{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Two fold cross validation for logistic regression and baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'heart_failure_clinical_records_dataset.csv'\n",
    "df = pd.read_csv(filename)\n",
    "y = df['DEATH_EVENT']\n",
    "X = df.drop(['DEATH_EVENT'], axis = 1)\n",
    "attributeNames = list(X.keys())\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "X = (X - mu) / sigma\n",
    "X = (X - mu) / sigma\n",
    "classNames = \"DEATH_EVENT\"\n",
    "N, M = X.shape\n",
    "C = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our models\n",
    "\n",
    "We are going to define two models:\n",
    "\n",
    "- A baseline\n",
    "- A logistic regression \n",
    "- A neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our dataset there are more entries of the class 0 (Survived)\n",
    "# The best possible accuracy will be around 70% given our dataset\n",
    "def baseline(dataset):\n",
    "    dataset_index = dataset.index\n",
    "    baseline = pd.Series(np.zeros(300)).astype(int)\n",
    "    baseline = baseline.iloc[dataset_index]\n",
    "    return baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_reg = lambda regularization: LogisticRegression(penalty=\"l2\", C= 1/regularization, max_iter= 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(model, X, y, X_test, y_test, X_par, regularization):\n",
    "    test_error = np.empty(len(regularization))\n",
    "    for s, lambda_value in enumerate(regularization):\n",
    "        reg = model(lambda_value)\n",
    "        reg.fit(X,y)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        error = sum(y_pred != y_test) / len(y_test)\n",
    "        test_error[s] = error\n",
    "    \n",
    "    return test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = lambda n_hidden_units: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Sigmoid() # final tranfer function\n",
    "                    )\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_net(model, loss_fn, X, y, n_hidden_units,\n",
    "                     n_replicates=1, max_iter = 10000, tolerance=1e-6):\n",
    "    # Specify maximum number of iterations for training\n",
    "    logging_frequency = 1000 # display the loss every 1000th iteration\n",
    "    best_final_loss = 1e100\n",
    "    for r in range(n_replicates):\n",
    "        # Make a new net (calling model() makes a new initialization of weights) \n",
    "        net = model(n_hidden_units)\n",
    "        \n",
    "        # initialize weights based on limits that scale with number of in- and\n",
    "        # outputs to the layer, increasing the chance that we converge to \n",
    "        # a good solution\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "                     \n",
    "        # We can optimize the weights by means of stochastic gradient descent\n",
    "        # The learning rate, lr, can be adjusted if training doesn't perform as\n",
    "        # intended try reducing the lr. If the learning curve hasn't converged\n",
    "        # (i.e. \"flattend out\"), you can try try increasing the maximum number of\n",
    "        # iterations, but also potentially increasing the learning rate:\n",
    "        #optimizer = torch.optim.SGD(net.parameters(), lr = 5e-3)\n",
    "        \n",
    "        # A more complicated optimizer is the Adam-algortihm, which is an extension\n",
    "        # of SGD to adaptively change the learing rate, which is widely used:\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        # Train the network while displaying and storing the loss\n",
    "        learning_curve = [] # setup storage for loss at each step\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X) # forward pass, predict labels on training set\n",
    "            loss = loss_fn(y_est, y) # determine loss\n",
    "            loss_value = loss.data.numpy() #get numpy array instead of tensor\n",
    "            learning_curve.append(loss_value) # record loss for later display\n",
    "            \n",
    "            # Convergence check, see if the percentual loss decrease is within\n",
    "            # tolerance:\n",
    "            p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "            \n",
    "            # display loss with some frequency:\n",
    "            # do backpropagation of loss and optimize weights \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            \n",
    "            \n",
    "        # display final loss\n",
    "        \n",
    "        if loss_value < best_final_loss: \n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "            best_hidden_unit = n_hidden_units\n",
    "        \n",
    "    # Return the best curve along with its final loss and learing curve\n",
    "    return best_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation outer fold: 1/3\n",
      "\n",
      "Crossvalidation inner fold: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([132])) that is different to the input size (torch.Size([132, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([199])) that is different to the input size (torch.Size([199, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation outer fold: 2/3\n",
      "\n",
      "Crossvalidation inner fold: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([132])) that is different to the input size (torch.Size([132, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([199])) that is different to the input size (torch.Size([199, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation outer fold: 3/3\n",
      "\n",
      "Crossvalidation inner fold: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([133])) that is different to the input size (torch.Size([133, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation inner fold: 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([134])) that is different to the input size (torch.Size([134, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/erik/anaconda3/envs/ann/lib/python3.7/site-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y.squeeze()\n",
    "\n",
    "# Variables to control fold splits\n",
    "K1 = 10\n",
    "K2 = 10\n",
    "\n",
    "# Initialize variables\n",
    "lambda_interval = np.linspace(0.001, 0.1, 9) # Holds regularization values for our logistic regression model\n",
    "hidden_units_interval = [1, 2, 3, 4, 5] # Holds hidden units range for our neural network\n",
    "final_models_lambda = []\n",
    "final_models_reg_error = []\n",
    "final_models_hidden_unit = []\n",
    "final_models_nn_error = []\n",
    "final_baseline_error = []\n",
    "Error_test = np.empty((K1,1))\n",
    "Validation_error_nn = np.zeros((len(hidden_units_interval), K2))\n",
    "Validation_error_reg = np.zeros((len(lambda_interval), K2)) \n",
    "\n",
    "Outer_fold = StratifiedKFold(n_splits = K1, shuffle = True)\n",
    "Inner_fold =  StratifiedKFold(n_splits = K2, shuffle = True)\n",
    "\n",
    "for i, (par_index, test_index) in enumerate(Outer_fold.split(X,y)):\n",
    "    print('\\nCrossvalidation outer fold: {0}/{1}'.format(i+1,K1))\n",
    "    X_par, y_par = X[par_index,:], y[par_index]\n",
    "    X_test, y_test = X[test_index,:], y[test_index]\n",
    "\n",
    "    Generalization_error_reg = []\n",
    "    Generalization_error_nn = []\n",
    "    Generalization_error_base = []\n",
    "\n",
    "\n",
    "    for j, (train_index, val_index) in enumerate(Inner_fold.split(X_par, y_par)):\n",
    "        print('\\nCrossvalidation inner fold: {0}/{1}'.format(j+1,K2))\n",
    "        X_train, y_train = X_par[train_index, :], y_par[train_index]\n",
    "        X_val, y_val = X_par[val_index], y_par[val_index]\n",
    "\n",
    "        # Now we test our s models in regression\n",
    "        for s, lambda_val in enumerate(lambda_interval):\n",
    "            model  = log_reg(lambda_val)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            Validation_error_reg[s,j] = (len(y_val)/len(y_par))*(sum(y_pred != y_val)/len(y_pred))\n",
    "        \n",
    "        X_train_t = torch.tensor(X_train, dtype= torch.float)\n",
    "        y_train_t = torch.tensor(y_train, dtype= torch.float)\n",
    "        X_val_t = torch.tensor(X_val, dtype= torch.float)\n",
    "        y_val_t = torch.tensor(y_val, dtype= torch.float)\n",
    "        # Test for each hidden unit\n",
    "        for h, n_hidden_units in enumerate(hidden_units_interval):\n",
    "            net = train_neural_net(nn, loss_fn, X_train_t, y_train_t, n_hidden_units)\n",
    "            y_sigmoid = net(X_val_t)\n",
    "            y_test_est = (y_sigmoid>.5).type(dtype=torch.uint8)\n",
    "            y_test_est = y_test_est.numpy()\n",
    "            y_test_est = y_test_est.reshape(y_val.shape)\n",
    "            # Determine errors and errors\n",
    "            Validation_error_nn[h,j] = (len(y_val)/len(y_par)) * (sum(y_test_est != y_val)/len(y_val))\n",
    "            \n",
    "    \n",
    "    # We compute for each model its generalization error\n",
    "    for s in range(len(lambda_interval)):\n",
    "        Generalization_error_reg.append(np.sum(Validation_error_reg[s, :]))\n",
    "\n",
    "    for h in range(len(hidden_units_interval)):\n",
    "        Generalization_error_nn.append(np.sum(Validation_error_nn[h, :]))\n",
    "\n",
    "    # We select the best model and compute its test error with D_test\n",
    "    min_index_reg = np.argmin(Generalization_error_reg)\n",
    "    best_lambda = lambda_interval[min_index_reg]\n",
    "    model = log_reg(best_lambda)\n",
    "    model.fit(X_par, y_par)\n",
    "    y_pred = model.predict(X_test)\n",
    "    error_test_reg = sum(y_pred != y_test)/len(y_pred)\n",
    "\n",
    "    X_par_t = torch.tensor(X_par, dtype= torch.float)\n",
    "    y_par_t = torch.tensor(y_par, dtype= torch.float)\n",
    "    X_test_t = torch.tensor(X_test, dtype= torch.float)\n",
    "    y_test_t = torch.tensor(y_test, dtype= torch.float)\n",
    "\n",
    "    min_index_nn = np.argmin(Generalization_error_nn)\n",
    "    best_hidden_unit = hidden_units_interval[min_index_nn]\n",
    "    net = train_neural_net(nn, loss_fn, X_par_t, y_par_t, best_hidden_unit)\n",
    "    y_sigmoid = net(X_test_t)\n",
    "    y_test_est = (y_sigmoid>.5).type(dtype=torch.uint8)\n",
    "    y_test_est = y_test_est.numpy()\n",
    "    y_test_est = y_test_est.reshape(y_test.shape)\n",
    "    error_test_nn = sum(y_test_est != y_test)/len(y_test)\n",
    "\n",
    "    y_pred_baseline = baseline(pd.Series(y_test))\n",
    "    \n",
    "    final_baseline_error.append(sum(y_pred_baseline != y_test)/len(y_pred_baseline))\n",
    "\n",
    "    # We store the final models data\n",
    "    final_models_lambda.append(best_lambda)\n",
    "    final_models_reg_error.append(error_test_reg)\n",
    "    final_models_hidden_unit.append(best_hidden_unit)\n",
    "    final_models_nn_error.append(error_test_nn)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 3)\n",
      "   Outer fold  h  E_test_nn   lambda  E_test_reg  E_test_baseline\n",
      "0           1  5   0.230000  0.00100     0.18000         0.320000\n",
      "1           2  4   0.330000  0.00100     0.14000         0.320000\n",
      "2           3  5   0.343434  0.02575     0.30303         0.323232\n"
     ]
    }
   ],
   "source": [
    "data = {\"Outer fold\": range(1, K1 + 1),\n",
    "        \"h\": final_models_hidden_unit,\n",
    "        \"E_test_nn\": final_models_nn_error,\n",
    "        \"lambda\": final_models_lambda,\n",
    "        \"E_test_reg\": final_models_reg_error,\n",
    "        \"E_test_baseline\": final_baseline_error}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index(\"Outer fold\")\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8b4e3371b54d7388ec29d45549dbada5a2ead940d1a974c186be5493bc53cc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
