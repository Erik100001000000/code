{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Two fold cross validation for logistic regression and baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'heart_failure_clinical_records_dataset.csv'\n",
    "df = pd.read_csv(filename)\n",
    "y = df['DEATH_EVENT']\n",
    "X = df.drop(['DEATH_EVENT'], axis = 1)\n",
    "attributeNames = list(X.keys())\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "X = (X - mu) / sigma\n",
    "X = (X - mu) / sigma\n",
    "classNames = \"DEATH_EVENT\"\n",
    "N, M = X.shape\n",
    "C = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our models\n",
    "\n",
    "We are going to define two models:\n",
    "\n",
    "- A baseline\n",
    "- A logistic regression \n",
    "- A neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our dataset there are more entries of the class 0 (Survived)\n",
    "# The best possible accuracy will be around 70% given our dataset\n",
    "def baseline():\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_reg = lambda regularization: LogisticRegression(penalty=\"l2\", C= 1/regularization, max_iter= 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(model, X, y, X_test, y_test, regularization):\n",
    "    test_error = np.empty(len(regularization))\n",
    "    for s, lambda_value in enumerate(regularization):\n",
    "        reg = model(lambda_value)\n",
    "        reg.fit(X,y)\n",
    "        y_pred = reg.predict(X_test)\n",
    "        error = sum(y_pred != y_test) / len(y_test)\n",
    "        test_error[s] = error\n",
    "    \n",
    "    return test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = lambda n_hidden_units: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Sigmoid() # final tranfer function\n",
    "                    )\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.squeeze()\n",
    "\n",
    "# Variables to control fold splits\n",
    "K1 = 3\n",
    "K2 = 3\n",
    "\n",
    "# Initialize variables\n",
    "lambda_interval = np.linspace(0.001, 0.1, 9) # Holds regularization values for our logistic regression model\n",
    "hidden_units_interval = np.linspace(1, 6) # Holds hidden units range for our neural network\n",
    "final_models_lambda = []\n",
    "final_models_reg_error = []\n",
    "final_models_hidden_unit = []\n",
    "final_models_nn_error = []\n",
    "Error_test = np.empty((K1,1))\n",
    "Validation_error_nn = np.zeros((len(hidden_units_interval), K2))\n",
    "Validation_error_reg = np.zeros((len(lambda_interval), K2)) \n",
    "\n",
    "Outer_fold = StratifiedKFold(n_splits = K1, shuffle = True)\n",
    "Inner_fold =  StratifiedKFold(n_splits = K2, shuffle = True)\n",
    "\n",
    "for i, (par_index, test_index) in enumerate(Outer_fold.split(X,y)):\n",
    "    X_par, y_par = X[par_index,:], y[par_index]\n",
    "    X_test, y_test = X[test_index,:], y[test_index]\n",
    "\n",
    "    Generalization_error_reg = []\n",
    "    Generalization_error_nn = []\n",
    "    Generalization_error_base = []\n",
    "\n",
    "\n",
    "    for j, (train_index, val_index) in enumerate(Inner_fold.split(X_par, y_par)):\n",
    "        X_train, y_train = X_par[train_index, :], y_par[train_index]\n",
    "        X_val, y_val = X_par[val_index], y_par[val_index]\n",
    "\n",
    "        # Now we test our s models in regression\n",
    "        for s, lambda_val in enumerate(lambda_interval):\n",
    "            model  = log_reg(lambda_val)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            Validation_error_reg[s,j] = (len(y_val)/len(y_par))*(sum(y_pred != y_val)/len(y_pred))\n",
    "\n",
    "    # We compute for each model its generalization error\n",
    "    for s in range(len(lambda_interval)):\n",
    "        Generalization_error_reg.append(np.sum(Validation_error_reg[s, :]))\n",
    "\n",
    "    # We select the best model and compute its test error with D_test\n",
    "    min_index_reg = np.argmin(Generalization_error_reg)\n",
    "    best_lambda = lambda_interval[min_index_reg]\n",
    "    model = log_reg(best_lambda)\n",
    "    model.fit(X_par, y_par)\n",
    "    y_pred = model.predict(X_test)\n",
    "    error_test_reg = sum(y_pred != y_test)/len(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    # We store the final models data\n",
    "    final_models_lambda.append(best_lambda)\n",
    "    final_models_reg_error.append(error_test_reg)\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8b4e3371b54d7388ec29d45549dbada5a2ead940d1a974c186be5493bc53cc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
